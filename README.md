# Vision-Transformers-for-Flowers
The Objective is to pre-train and finetune vision transformers on the Flowers-102 Dataset, incorporate the Masked Image Modeling pre-text task, and explore  different variants of Masked Image Modelling.

The Oxford Flowers-102 Dataset is utilized to pre-train the Vision Transformer models incorporating Masked Image Modelling pre-task.

More information on the Dataset and data set split can be found in [`dataset`](https://github.com/narasimhan3737/Vision-Transformers-for-Flowers/tree/main/dataset) folder

## Analysis of Vision transformer models 

The pretrained Vistion Transformer models are analysed by taking self-attention maps, which are visualized in the [`analysis`](https://github.com/narasimhan3737/Vision-Transformers-for-Flowers/tree/main/analysis) folder

## Evaluation of Vision transformer models 

The pretrained Vision Transformer models are evaluated using K-NN classification and Linear Probing.

The results of the evaluations can be found in the [`evaluation`](https://github.com/narasimhan3737/Vision-Transformers-for-Flowers/tree/main/evaluation) folder

